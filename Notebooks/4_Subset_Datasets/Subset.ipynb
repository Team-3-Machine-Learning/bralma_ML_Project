{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aabc770d",
   "metadata": {},
   "source": [
    "# Subset of dataset\n",
    "### Create subset from 2001 to 2017.\n",
    "We take all rows where year is between 2001 and 2017, so we only use the years that both dataset have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17668d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path_dataset1 = \"../../Dataset1/Cleaned/electricity_demand_2001_2025_NA.csv\"\n",
    "#path_dataset1 = \"../../Dataset1/Cleaned/electricity_demand_2001_2025_cleaned.csv\"\n",
    "\n",
    "path_dataset1_out = \"../../Dataset1/Subset/electricity_demand_2001_2025_subset.csv\"\n",
    "\n",
    "path_dataset2 = \"../../Dataset2/Cleaned/price_paid_records_NA.csv\"\n",
    "#path_dataset2 = \"../../Dataset2/Cleaned/price_paid_records_cleaned.csv\"\n",
    "\n",
    "path_dataset2_out = \"../../Dataset2/Subset/price_paid_records_subset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f8d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(path_dataset1)\n",
    "df2 = pd.read_csv(path_dataset2)\n",
    "\n",
    "df2['date_of_transfer'] = pd.to_datetime(df2['date_of_transfer'], errors='coerce')\n",
    "df2['year'] = df2['date_of_transfer'].dt.year\n",
    "\n",
    "df1_filtered = df1[(df1['year'] >= 2001) & (df1['year'] <= 2017)]\n",
    "df2_filtered = df2[(df2['year'] >= 2001) & (df2['year'] <= 2017)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4cca59",
   "metadata": {},
   "source": [
    "### Remove unused columns\n",
    "We drop the columns that are always NA in our subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceed9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop1 = ['ifa_flow','nsl_flow', 'eleclink_flow', 'viking_flow', 'greenlink_flow']\n",
    "df1_filtered = df1_filtered.drop(columns=columns_to_drop1, errors='ignore')\n",
    "\n",
    "columns_to_drop2 = ['transaction_unique_identifier']\n",
    "df2_filtered = df2_filtered.drop(columns=columns_to_drop2, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4203e5b8",
   "metadata": {},
   "source": [
    "### Remove redundant columns\n",
    "Some columns are not relevant for helping our model predict * *row we will predict* *, so we drop these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7bbfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- property_type ---\n",
      "['F' 'D' 'S' 'T' 'O']\n",
      "\n",
      "--- old/new ---\n",
      "['N' 'Y']\n",
      "\n",
      "--- duration ---\n",
      "['L' 'F' 'U']\n",
      "\n",
      "--- ppdcategory_type ---\n",
      "['A' 'B']\n",
      "\n",
      "--- record_status_-_monthly_file_only ---\n",
      "['A']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "    \"property_type\",\n",
    "    \"old/new\",\n",
    "    \"duration\",\n",
    "    \"ppdcategory_type\",\n",
    "    \"record_status_-_monthly_file_only\"\n",
    "]\n",
    "\n",
    "for c in cols:\n",
    "    print(f\"--- {c} ---\")\n",
    "    print(df2_filtered[c].unique())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb7d22",
   "metadata": {},
   "source": [
    "We can see that \"record_status_-_monthly_file_only\" only has one value, which is 'A'. That is why we decide to drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a315d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price', 'date_of_transfer', 'property_type', 'old/new', 'duration',\n",
      "       'town/city', 'district', 'county', 'ppdcategory_type', 'year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2_filtered = df2_filtered.drop(columns=['record_status_-_monthly_file_only'], errors='ignore')\n",
    "print(df2_filtered.columns)\n",
    "\n",
    "\n",
    "df1_filtered.to_csv(path_dataset1_out, index=False)\n",
    "df2_filtered.to_csv(path_dataset2_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
